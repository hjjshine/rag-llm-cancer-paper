#utils/evaluation.py
import sys
import os
root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(root_dir)
from utils.io import load_object
import re
from typing import Union, Dict, List
import matplotlib.pyplot as plt
import seaborn as sns
from math import pi


# Load brand-generic name mapping
drug_names_mapping_dict=load_object(filename=os.path.join(root_dir,'data/fda_drug_names_mapping_dict.pkl'))

# Load query-groundtruth drug mapping
all_generic_names_set=load_object(os.path.join(root_dir,"data/latest_db/all_generic_drugs.pkl"))


# Get selected evaluation metric results from each run
def get_selected_metrics(output_eval: dict, metrics: Union[str, List[str]], multiply: bool = True):
    if isinstance(metrics, str):
        metrics = [metrics]
    factor = 100 if multiply else 1
    return {m: output_eval.get(m)*factor for m in metrics}


# Function to get average evaluation results from multiple runs
def get_avg_eval_runs(
    eval_output: list[dict], 
    num_runs: int, 
    metrics: list[str], 
    get_metrics=get_selected_metrics
    ):
    #1) get selected metric values from each run
    all_eval_output=[get_metrics(eval_output[i], metrics) for i in range(num_runs)]
    #2) get average of selected metric values across multiple runs
    avg_all_eval_output={m: np.array([res[m] for res in all_eval_output]).mean() for m in metrics}
    
    return avg_all_eval_output


# Function to extract drug names
def extract_drug_names(drug_lines):
    pattern = r'"drug names?":\s?(?:\[(.*?)\]|"(.*?)"|([^"\[\]]+))'
    cleaned_drug_list = []
    for line in drug_lines:
        match = re.search(pattern, line)
        if match:
            drug_string = next(filter(None, match.groups())) # Extract the first non-empty capturing group
            drug_string = re.sub(r'\s?\([^)]*\)', '', drug_string) # Remove any text within parentheses
            drugs = [drug.strip().strip('"').replace('Â®', '') for drug in re.split(r'\s*\+\s*|\s*,\s*', drug_string)] # Split by `+` or `,` and clean spaces & quotes
            cleaned_drug_list.append(set(drugs))
    return(cleaned_drug_list)


# Function to check if the model explicitly says there are no FDA-approved drugs.
def is_no_drug_output(output: str) -> bool:
    no_drug_phrases = [
        "no fda-approved",
        "no specific",
        "no approved",
        "no therapies",
        "no therapy",
        "no drugs",
        "no drug",
        "not specified",
        "not approved",
        "not available",
        "none"
        ]
    return any(phrase in output.lower() for phrase in no_drug_phrases)


# Function to evaluate predicted drugs from synthetic queries
def calc_eval_metrics(
    output_test_ls: list[str], 
    query_ls: list[str], 
    prompt_groundtruth_dict: dict[str, list]
    ) -> dict:
    """
    Evaluate predicted drug from LLM.
    
    Arguments: 
        output_test_ls (list): List of full output generated by LLM.
        query_ls (list): List of input prompts.
        prompt_groundtruth_dict (dict): Dictionary of input prompts and matching ground-truth drugs.
    """
    exact_match_acc, partial_match_acc = [], []
    precision_ls, recall_ls, f1_ls, specificity_ls = [], [], [], []
    pred_drugs_generic_set_ls = []
    true_drugs_generic_set_ls = []
    
    for i, output in enumerate(output_test_ls):
        
        # Extract drug name lines
        s_split=output.split("\n") # Split each line
        drug_lines=[line.lower() for line in s_split if "Drug Name" in line] # Extract relevant lines
        
        # If the model explicitly returned "none" or "no fda-approved drugs", treat it as empty
        if any(is_no_drug_output(drug) for drug in drug_lines):
            pred_drugs_names_set = set()
        else:
            pred_drugs_names_set = extract_drug_names(drug_lines) # Create a set of predicted individual drugs
        
        # Convert brand names to generic names if there are matching brand names, otherwise just append
        pred_drugs_generic_list=[]
        for subset in pred_drugs_names_set:
            normalized_subset_list=[]
            if all(drug not in drug_names_mapping_dict for drug in subset): # if all drugs are not a brand name in our mapping dict
                normalized_subset_list.extend([subset])
            else:
                for drug in subset:
                    if drug in drug_names_mapping_dict:
                        generic_names=drug_names_mapping_dict[drug]
                        normalized_subset_list.extend(generic_names) 
                    else:
                        normalized_subset_list.extend([{drug}])
            pred_drugs_generic_list.append(normalized_subset_list)
        
        # Convert the list of generic names to unique frozensets 
        pred_drugs_generic_set=set()
        for lst in pred_drugs_generic_list:
            if len(lst) == 1:
                pred_drugs_generic_set.add(frozenset(*lst))
            else:
                for subset in lst:
                    pred_drugs_generic_set.add(frozenset(subset))
        pred_drugs_generic_set_ls.append(pred_drugs_generic_set)
        
        # Convert the list of ground-truth generic names to unique frozensets
        true_drugs_generic_set = {frozenset(_set) for _set in prompt_groundtruth_dict[query_ls[i]]}
        true_drugs_generic_set_ls.append(true_drugs_generic_set)
        
        # Cases with ground-truth therapies
        if len(true_drugs_generic_set) != 0:
            
            # Compute exact match accuracy (if all true drugs are in the predicted drug output)
            exact_match_acc.append(all(subset in pred_drugs_generic_set for subset in true_drugs_generic_set))
            
            # Compute partial match accuracy (if one or more true drugs are in the predicted drug output)
            partial_match_acc.append(len(pred_drugs_generic_set & true_drugs_generic_set) > 0)
            
            # All possible FDA-approved drugs
            all_drugs_set = true_drugs_generic_set | pred_drugs_generic_set | all_generic_names_set
            
            # Calculate true positive, false positive, false negative, true negative
            tp = len(pred_drugs_generic_set.intersection(true_drugs_generic_set))  
            fp = len(pred_drugs_generic_set - true_drugs_generic_set)  
            fn = len(true_drugs_generic_set - pred_drugs_generic_set)  
            tn = len(all_drugs_set - true_drugs_generic_set - pred_drugs_generic_set)  
            
            # Calculate precision, recall, and F1 scores
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  
            precision_ls.append(precision)
            recall_ls.append(recall)
            f1_ls.append(f1)
            specificity_ls.append(specificity) 
        
        # Cases with no ground-truth therapies (no FDA-approved drugs available)
        else:
            partial_match_acc.append(True if not true_drugs_generic_set and not pred_drugs_generic_set else False) 
            exact_match_acc.append(True if not true_drugs_generic_set and not pred_drugs_generic_set else False) 
            precision_ls.append(None)
            recall_ls.append(None)
            f1_ls.append(None)
            specificity_ls.append(None)

    def filtered_res(lst):
            filtered_res = [x for x in lst if x is not None]  
            return(filtered_res)

    avg_exact_match_acc=sum(x is True for x in exact_match_acc)/len(exact_match_acc)
    avg_partial_match_acc=sum(x is True for x in partial_match_acc)/len(partial_match_acc)
    filtered_precision=filtered_res(precision_ls)
    filtered_recall=filtered_res(recall_ls)
    filtered_f1=filtered_res(f1_ls)
    filtered_specificity=filtered_res(specificity_ls)
    avg_precision=sum(filtered_precision)/len(filtered_precision)
    avg_recall=sum(filtered_recall)/len(filtered_recall)
    avg_f1=sum(filtered_f1)/len(filtered_f1)
    avg_specificity=sum(filtered_specificity)/len(filtered_specificity)

    result={
        'avg_exact_match_acc':avg_exact_match_acc,
        'avg_partial_match_acc':avg_partial_match_acc,
        'avg_precision':avg_precision,
        'avg_recall':avg_recall,
        'avg_f1':avg_f1,
        'avg_specificity':avg_specificity,
        'exact_match_acc':exact_match_acc,
        'partial_match_acc':partial_match_acc,
        'precision_ls':precision_ls,
        'recall_ls':recall_ls,
        'f1_ls':f1_ls,
        'specificity_ls':specificity_ls,
        'pred_drugs_generic_set_ls':pred_drugs_generic_set_ls,
        'true_drugs_generic_set_ls':true_drugs_generic_set_ls
    }
    
    return(result)


# Function for radar chart
def plot_radar_chart(
    eval_results: Dict[str, dict],
    metrics: List[str],
    metric_labels: Dict[str, str], # mapping from metric key -> pretty label
    title: str,
    colors: List[str] = None,
    ylim: int = 100,
    annotate: bool = True,
    save_path: str = None
):
    """
    Plots a radar chart for one or more models.

    Args:
        eval_results: dict mapping model name -> evaluation dictionary
        metrics: list of metric keys to include (in order)
        metric_labels: dict mapping metric key -> display label
    """
    # set up colors
    if colors is None:
        cols = sns.color_palette(cc.glasbey, n_colors=len(eval_results))
    else:
        cols = colors

    # prepare metric labels + angles
    labels = [metric_labels[m] for m in metrics]
    angles = [n / float(len(metrics)) * 2 * pi for n in range(len(metrics))]
    angles += angles[:1]  # close the loop

    # start figure
    plt.figure(figsize=(5, 5))
    ax = plt.subplot(111, polar=True)

    # plot each model
    for i, (name, results) in enumerate(eval_results.items()):
        values = [results.get(m, 0) for m in metrics]
        values += values[:1]
        ax.plot(angles, values, color=cols[i], linewidth=2, linestyle="solid", label=name)
        
        # annotate numbers
        if annotate:
            for j, v in enumerate(values[:-1]):
                offset = -ylim*0.08 if i == 0 else ylim*0.08
                ax.text(
                    angles[j], v + offset,
                    f"{v:.0f}", color="black", fontsize=10,
                    ha="center", va="center"
                )

    # configure axes
    ax.set_ylim(0, ylim)
    ax.set_xticks([])
    ax.tick_params(axis='y', labelsize=9)
    for i, label in enumerate(labels):
        angle = angles[i]
        if angle == 0:  
            ax.text(angle, 160, label, 
                    horizontalalignment='center', verticalalignment='center', 
                    fontsize=17)       
        else:
            ax.text(angle, 130, label, 
                    horizontalalignment='center', 
                    verticalalignment='center', 
                    fontsize=17)

    # legend
    ax.legend(
        loc="upper center", bbox_to_anchor=(0.5, -0.12),
        ncol=2, frameon=False, fontsize=11
    )

    plt.title(title, fontsize=14, pad=20)

    # save or show
    if save_path:
        plt.savefig(save_path, dpi=600, bbox_inches="tight")
    else:
        plt.show()
