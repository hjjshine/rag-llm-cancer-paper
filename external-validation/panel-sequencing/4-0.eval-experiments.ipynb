{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f953c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pandas as pd, ast, json\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a492c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d797e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== GENERAL IMPORTS ==================\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ================== UTIL FUNCTIONS ==================\n",
    "from utils.embedding import get_context_db, retrieve_context\n",
    "from utils.prompt import get_prompt\n",
    "from llm.run_RAGLLM import run_RAG\n",
    "\n",
    "\n",
    "# ================== MODEL & API IMPORTS ==================\n",
    "from mistralai.client import MistralClient\n",
    "from openai import OpenAI\n",
    "from llm.inference import run_llm\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c24055",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d89f7ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regorafenib'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#answers\n",
    "#gold answer is\n",
    "df = pd.read_csv('external-validation/panel-sequencing/reports/filtered-data/first_line_treatments_post_msk.csv')\n",
    "answers = []\n",
    "for f in glob('external-validation/panel-sequencing/reports/test/*.txt'):\n",
    "    patient = f.split('/')[-1].replace('.txt','')\n",
    "    treatment = {i.lower() for i in ast.literal_eval(df[df['PATIENT_ID'] == patient]['FIRST_LINE_TREATMENT'].iloc[0])}\n",
    "    answers.append([patient, treatment])\n",
    "answers = pd.DataFrame(answers, columns=['PATIENT_ID', 'FIRST_LINE_TREATMENT'])\n",
    "answers['FIRST_LINE_TREATMENT'] = answers['FIRST_LINE_TREATMENT']\n",
    "answers = dict(zip(answers['PATIENT_ID'], answers['FIRST_LINE_TREATMENT']))\n",
    "answers[patient]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ce4f2",
   "metadata": {},
   "source": [
    "----\n",
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ae65a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils/evaluation.py\n",
    "import sys\n",
    "import os\n",
    "script_dir = os.getcwd()\n",
    "root_dir = script_dir#os.path.join(os.path.dirname(os.path.abspath(script_dir)))\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.abspath(script_dir))))\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from utils.io import load_object\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d572b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load brand-generic name mapped dictionary and prompt-groundtruth (generic) mapped dictionary\n",
    "drug_names_mapping_dict=load_object(filename=os.path.join(root_dir,'data/fda_drug_names_mapping_dict.pkl'))\n",
    "\n",
    "# Extract ground-truth generic and brand names\n",
    "all_generic_names_set=set()\n",
    "for lst in drug_names_mapping_dict.values():\n",
    "    for name in lst:\n",
    "        all_generic_names_set.add(frozenset(name))\n",
    "\n",
    "# Function to extract drug names\n",
    "def extract_drug_names(drug_lines):\n",
    "    pattern = r'\"drug names?\":\\s?(?:\\[(.*?)\\]|\"(.*?)\"|([^\"\\[\\]]+))'\n",
    "    cleaned_drug_list = []\n",
    "    for line in drug_lines:\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            drug_string = next(filter(None, match.groups())) # Extract the first non-empty capturing group\n",
    "            drug_string = re.sub(r'\\s?\\([^)]*\\)', '', drug_string) # Remove any text within parentheses\n",
    "            drugs = [drug.strip().strip('\"').replace('Â®', '') for drug in re.split(r'\\s*\\+\\s*|\\s*,\\s*', drug_string)] # Split by `+` or `,` and clean spaces & quotes\n",
    "            cleaned_drug_list.append(set(drugs))\n",
    "    return(cleaned_drug_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ce2f3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the model explicitly says there are no FDA-approved drugs.\n",
    "def is_no_drug_output(output: str) -> bool:\n",
    "    no_drug_phrases = [\"no fda-approved drugs\", \"none\", \"no approved therapies\", \"no therapies available\"]\n",
    "    return any(phrase in output.lower() for phrase in no_drug_phrases)\n",
    "\n",
    "# Function to evaluate predicted drugs from synthetic queries\n",
    "def calc_eval_metrics(\n",
    "    output_test_ls: list[str], \n",
    "    query_ls: list[str], \n",
    "    prompt_groundtruth_dict: dict[str, list]\n",
    "    ) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate predicted drug from LLM.\n",
    "    \n",
    "    Arguments: \n",
    "        output_test_ls (list): List of full output generated by LLM.\n",
    "        query_ls (list): List of input prompts.\n",
    "        prompt_groundtruth_dict (dict): Dictionary of input prompts and matching ground-truth drugs.\n",
    "    \n",
    "    \"\"\"\n",
    "    exact_match_acc, partial_match_acc = [], []\n",
    "    precision_ls, recall_ls, f1_ls, specificity_ls = [], [], [], []\n",
    "    pred_drugs_generic_set_ls = []\n",
    "    true_drugs_generic_set_ls = []\n",
    "    \n",
    "    for i, output in enumerate(output_test_ls):\n",
    "        \n",
    "        # Extract drug name lines\n",
    "        s_split=output.split(\"\\n\") # Split each line\n",
    "        drug_lines=[line.lower() for line in s_split if \"Drug Name\" in line] # Extract relevant lines\n",
    "        \n",
    "        # If the model explicitly returned \"none\" or \"no fda-approved drugs\", treat it as empty\n",
    "        if any(is_no_drug_output(drug) for drug in drug_lines):\n",
    "            pred_drugs_names_set = set()\n",
    "        else:\n",
    "            pred_drugs_names_set = extract_drug_names(drug_lines) # Create a set of predicted individual drugs\n",
    "        \n",
    "        # Convert brand names to generic names if there are matching brand names, otherwise just append\n",
    "        pred_drugs_generic_list=[]\n",
    "        for subset in pred_drugs_names_set:\n",
    "            normalized_subset_list=[]\n",
    "            if all(drug not in drug_names_mapping_dict for drug in subset): # if all drugs are not a brand name in our mapping dict\n",
    "                normalized_subset_list.extend([subset])\n",
    "            else:\n",
    "                for drug in subset:\n",
    "                    if drug in drug_names_mapping_dict:\n",
    "                        generic_names=drug_names_mapping_dict[drug]\n",
    "                        normalized_subset_list.extend(generic_names) \n",
    "                    else:\n",
    "                        normalized_subset_list.extend([{drug}])\n",
    "            pred_drugs_generic_list.append(normalized_subset_list)\n",
    "        \n",
    "        # Convert the list of generic names to unique frozensets \n",
    "        pred_drugs_generic_set=set()\n",
    "        for lst in pred_drugs_generic_list:\n",
    "            if len(lst) == 1:\n",
    "                pred_drugs_generic_set.add(frozenset(*lst))\n",
    "            else:\n",
    "                for subset in lst:\n",
    "                    pred_drugs_generic_set.add(frozenset(subset))\n",
    "        pred_drugs_generic_set_ls.append(pred_drugs_generic_set)\n",
    "        \n",
    "        # Convert the list of ground-truth generic names to unique frozensets\n",
    "        true_drugs_generic_set = {frozenset(_set) for _set in prompt_groundtruth_dict[query_ls[i]]}\n",
    "        true_drugs_generic_set_ls.append(true_drugs_generic_set)\n",
    "        \n",
    "        # Cases with ground-truth therapies\n",
    "        if len(true_drugs_generic_set) != 0:\n",
    "            \n",
    "            # Compute exact match accuracy (if all true drugs are in the predicted drug output)\n",
    "            exact_match_acc.append(all(subset in pred_drugs_generic_set for subset in true_drugs_generic_set))\n",
    "            \n",
    "            # Compute partial match accuracy (if one or more true drugs are in the predicted drug output)\n",
    "            partial_match_acc.append(len(pred_drugs_generic_set & true_drugs_generic_set) > 0)\n",
    "            \n",
    "            # All possible FDA-approved drugs\n",
    "            all_drugs_set = true_drugs_generic_set | pred_drugs_generic_set | all_generic_names_set\n",
    "            \n",
    "            # Calculate true positive, false positive, false negative, true negative\n",
    "            tp = len(pred_drugs_generic_set.intersection(true_drugs_generic_set))  \n",
    "            fp = len(pred_drugs_generic_set - true_drugs_generic_set)  \n",
    "            fn = len(true_drugs_generic_set - pred_drugs_generic_set)  \n",
    "            tn = len(all_drugs_set - true_drugs_generic_set - pred_drugs_generic_set)  \n",
    "            \n",
    "            # Calculate precision, recall, and F1 scores\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  \n",
    "            precision_ls.append(precision)\n",
    "            recall_ls.append(recall)\n",
    "            f1_ls.append(f1)\n",
    "            specificity_ls.append(specificity) \n",
    "        \n",
    "        # Cases with no ground-truth therapies (no FDA-approved drugs available)\n",
    "        else:\n",
    "            partial_match_acc.append(True if not true_drugs_generic_set and not pred_drugs_generic_set else False) \n",
    "            exact_match_acc.append(True if not true_drugs_generic_set and not pred_drugs_generic_set else False) \n",
    "            precision_ls.append(None)\n",
    "            recall_ls.append(None)\n",
    "            f1_ls.append(None)\n",
    "            specificity_ls.append(None)\n",
    "\n",
    "    def filtered_res(lst):\n",
    "            filtered_res = [x for x in lst if x is not None]  \n",
    "            return(filtered_res)\n",
    "\n",
    "    avg_exact_match_acc=sum(x is True for x in exact_match_acc)/len(exact_match_acc)\n",
    "    avg_partial_match_acc=sum(x is True for x in partial_match_acc)/len(partial_match_acc)\n",
    "    filtered_precision=filtered_res(precision_ls)\n",
    "    filtered_recall=filtered_res(recall_ls)\n",
    "    filtered_f1=filtered_res(f1_ls)\n",
    "    filtered_specificity=filtered_res(specificity_ls)\n",
    "    avg_precision=sum(filtered_precision)/len(filtered_precision)\n",
    "    avg_recall=sum(filtered_recall)/len(filtered_recall)\n",
    "    avg_f1=sum(filtered_f1)/len(filtered_f1)\n",
    "    avg_specificity=sum(filtered_specificity)/len(filtered_specificity)\n",
    "\n",
    "    result={\n",
    "        'avg_exact_match_acc':avg_exact_match_acc,\n",
    "        'avg_partial_match_acc':avg_partial_match_acc,\n",
    "        'avg_precision':avg_precision,\n",
    "        'avg_recall':avg_recall,\n",
    "        'avg_f1':avg_f1,\n",
    "        'avg_specificity':avg_specificity,\n",
    "        'exact_match_acc':exact_match_acc,\n",
    "        'partial_match_acc':partial_match_acc,\n",
    "        'precision_ls':precision_ls,\n",
    "        'recall_ls':recall_ls,\n",
    "        'f1_ls':f1_ls,\n",
    "        'specificity_ls':specificity_ls,\n",
    "        'pred_drugs_generic_set_ls':pred_drugs_generic_set_ls,\n",
    "        'true_drugs_generic_set_ls':true_drugs_generic_set_ls\n",
    "    }\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb86a3",
   "metadata": {},
   "source": [
    "---\n",
    "### analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d1221363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_output(file_path):\n",
    "    output_list = []\n",
    "    for f in glob(file_path):\n",
    "        with open(f, 'r') as file:\n",
    "            output_list.append(json.load(file)['response'])\n",
    "    return output_list\n",
    "def read_prompt(file_path):\n",
    "    prompt_list = []\n",
    "    for f in glob(file_path):\n",
    "        with open(f, 'r') as file:\n",
    "            prompt_list.append(json.load(file)['prompt'])\n",
    "    return prompt_list\n",
    "def read_truth(file_path):\n",
    "    truth = {}\n",
    "    for f in glob(file_path):\n",
    "        patient = f.split('/')[-1].replace('.json','')\n",
    "        with open(f, 'r') as file:\n",
    "            truth[json.load(file)['prompt']] = [answers[patient]]\n",
    "    return truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4a54feb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic-llm-prompt0\n",
      "Partial Match: 0.12244897959183673\n",
      "Exact Match: 0.12244897959183673\n",
      "=========\n",
      "basic-llm-prompt5\n",
      "Partial Match: 0.02040816326530612\n",
      "Exact Match: 0.02040816326530612\n",
      "=========\n",
      "basic-rag-prompt0\n",
      "Partial Match: 0.02040816326530612\n",
      "Exact Match: 0.02040816326530612\n",
      "=========\n",
      "basic-rag-prompt5\n",
      "Partial Match: 0.04081632653061224\n",
      "Exact Match: 0.04081632653061224\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "for file_path in ['external-validation/panel-sequencing/experiments/basic-llm-prompt0/*',\n",
    "                  'external-validation/panel-sequencing/experiments/basic-llm-prompt5/*',\n",
    "                  'external-validation/panel-sequencing/experiments/basic-rag-prompt0/*',\n",
    "                  'external-validation/panel-sequencing/experiments/basic-rag-prompt5/*']:\n",
    "    eval = calc_eval_metrics(read_output(file_path), read_prompt(file_path), read_truth(file_path))\n",
    "    print(file_path.split('/')[-2])\n",
    "    print('Partial Match:', eval['avg_partial_match_acc'])\n",
    "    print('Exact Match:', eval['avg_exact_match_acc'])\n",
    "    print('=========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7183e342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
