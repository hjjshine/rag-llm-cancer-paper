{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f953c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, pandas as pd, ast, json\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a492c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d797e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== GENERAL IMPORTS ==================\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ================== UTIL FUNCTIONS ==================\n",
    "from utils.embedding import get_context_db, retrieve_context\n",
    "from utils.prompt import get_prompt\n",
    "from llm.run_RAGLLM import run_RAG\n",
    "\n",
    "\n",
    "# ================== MODEL & API IMPORTS ==================\n",
    "from mistralai.client import MistralClient\n",
    "from openai import OpenAI\n",
    "from llm.inference import run_llm\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c24055",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c3db927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- existing module state ---\n",
    "_READY = False\n",
    "_CLIENT = None\n",
    "_CONTEXT = None\n",
    "_INDEX = None\n",
    "_MODEL_TYPE = None\n",
    "_MODEL_NAME = None\n",
    "_MODEL_EMBED = None\n",
    "\n",
    "def _cache_paths(embed_name: str, version: str = \"v1\"):\n",
    "    os.makedirs(\"indexes\", exist_ok=True)\n",
    "    return (\n",
    "        f\"indexes/{embed_name}__{version}.faiss\",\n",
    "        f\"indexes/{embed_name}__{version}.context.json\",\n",
    "    )\n",
    "\n",
    "def init(\n",
    "    context_json_path: str = \"data/structured_context_chunks.json\",\n",
    "    model_type: str = \"gpt\",\n",
    "    model_api: str = \"gpt-4o-2024-05-13\",\n",
    "    *,\n",
    "    force_rebuild: bool = False,\n",
    "):\n",
    "    global _READY, _CLIENT, _CONTEXT, _INDEX, _MODEL_TYPE, _MODEL_NAME, _MODEL_EMBED\n",
    "    if _READY:\n",
    "        return\n",
    "\n",
    "    load_dotenv()\n",
    "    if model_type in [\"gpt\", \"gpt_reasoning\"]:\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key: raise RuntimeError(\"OPENAI_API_KEY not set\")\n",
    "        _CLIENT = OpenAI(api_key=api_key)\n",
    "        _MODEL_EMBED = \"text-embedding-3-small\"\n",
    "    elif model_type in [\"mistral\", \"mistral-7b\"]:\n",
    "        api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "        if not api_key: raise RuntimeError(\"MISTRAL_API_KEY not set\")\n",
    "        _CLIENT = MistralClient(api_key=api_key)\n",
    "        _MODEL_EMBED = \"mistral-embed\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_type. Please choose from: mistral-7b, mistral, gpt\")\n",
    "\n",
    "    _MODEL_TYPE = model_type\n",
    "    _MODEL_NAME = model_api\n",
    "\n",
    "    index_path, ctx_path = _cache_paths(_MODEL_EMBED)\n",
    "\n",
    "    if (not force_rebuild) and os.path.exists(index_path) and os.path.exists(ctx_path):\n",
    "        with open(ctx_path, \"r\") as f:\n",
    "            _CONTEXT = json.load(f)\n",
    "        _INDEX = faiss.read_index(index_path)\n",
    "    else:\n",
    "        with open(context_json_path, \"r\") as f:\n",
    "            _CONTEXT = json.load(f)\n",
    "        _INDEX = get_context_db(_CONTEXT, _CLIENT, _MODEL_EMBED)\n",
    "        faiss.write_index(_INDEX, index_path)\n",
    "        with open(ctx_path, \"w\") as f:\n",
    "            json.dump(_CONTEXT, f)\n",
    "\n",
    "    _READY = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "608027e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(\n",
    "    text: str,\n",
    "    strategy: int = 0,\n",
    "    num_vec: int = 10,\n",
    "    max_len: int = 2048,\n",
    "    temp: float = 0.0,\n",
    "    random_seed: int = 2025,\n",
    "    *,\n",
    "    rag = True\n",
    "):\n",
    "    if not _READY:\n",
    "            raise RuntimeError(\"Call init(...) first.\")\n",
    "    if rag:\n",
    "        out, _ = run_RAG(\n",
    "            _CONTEXT,\n",
    "            text,\n",
    "            strategy,\n",
    "            _INDEX,\n",
    "            _CLIENT,\n",
    "            num_vec,\n",
    "            _MODEL_TYPE,\n",
    "            _MODEL_NAME,\n",
    "            _MODEL_EMBED,\n",
    "            max_len,\n",
    "            temp,\n",
    "            random_seed,\n",
    "        )\n",
    "    return out or \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ce4f2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9f5705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"external-validation/panel-sequencing/experiments/basic-rag-prompt5\", exist_ok=True)\n",
    "for f in glob('external-validation/panel-sequencing/reports/test/*.txt'):\n",
    "    patient = f.split('/')[-1].replace('.txt','')\n",
    "    \n",
    "    with open(f, 'r') as report_file:\n",
    "        report_content = report_file.read()\n",
    "    format_input = '''\n",
    "    The following is a summarized molecular profile from MSK-IMPACT.\n",
    "    Based on this profile, answer with the appropriate treatments for this patient in the format above.\n",
    "    {report}'''.format(report=report_content)\n",
    "    \n",
    "    init(context_json_path=\"data/structured_context_chunks.json\",\n",
    "        model_type=\"gpt\",\n",
    "        model_api=\"gpt-4.1-nano\",\n",
    "        force_rebuild=False)\n",
    "    \n",
    "    strategy = 5\n",
    "    prompt = get_prompt(strategy = strategy, prompt_chunk= format_input)\n",
    "    response = answer(prompt, strategy=strategy, rag=True)\n",
    "\n",
    "    with open('external-validation/panel-sequencing/experiments/basic-rag-prompt5/{p}.json'.format(p = patient), 'w') as f:\n",
    "        json.dump({\"prompt\": prompt, \"response\": response}, f, indent = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6bbcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85a2df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
